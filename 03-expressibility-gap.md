# The Expressibility Gap

> **Deep dive**: [github.com/MartyBonacci/expressibility-gap](https://github.com/MartyBonacci/expressibility-gap)

## The Problem

There's always a gap between:
- What we **envision** in our heads
- What we **communicate** to others

This isn't new. This isn't an AI problem. This is a **human** problem.

We can see the end goal and some steps along the path, but we most clearly see **the first step**.

## The Client/Developer Analogy

**Client:** "This isn't what I envisioned."
**Developer:** "This is exactly what you described."

Both are right.

The client had a full mental model. The developer built to the spec. The gap is in the **translation** from vision to words.

---

## Why We Don't Notice It

When you build something yourself, you fill in the gaps automatically:
- You know what you meant
- You correct course as you go
- You never have to explain it perfectly

But when you **hand it off** — to a developer, to a team, to an AI — the gap becomes visible.

---

## Two Types of Gap

### 1. Unknown Unknowns
You lack the conceptual framework to even perceive certain possibilities.

*Example*: Describing a login system without anticipating password resets, session timeouts, or OAuth flows.

### 2. Cognitive Overflow
You understand the components, but can't hold the entire complex system in working memory at once.

*This is a neurological constraint* — working memory holds about 7±2 items (Herbert Simon's bounded rationality).

---

## This Isn't a Bug

The expressibility gap is what allows us to **start big projects** without drowning in detail.

If you had to specify every pixel, every interaction, every edge case before starting, you'd never start.

Instead, you:
1. Describe the first step clearly enough
2. See what gets built
3. Course-correct based on reality
4. Repeat

The gap isn't a failure. It's **iterative development**.

---

## AI Makes It Obvious

AI exposes the expressibility gap because:
- It takes your words literally
- It doesn't "know what you meant"
- It forces you to **be more precise** or **iterate faster**

The same gap existed with human developers. We're just used to tolerating it there.

With AI, we have a choice:
- Get frustrated that it "doesn't understand"
- Or learn to **stack context** better

Guess which one works.

---

## The Critical Insight

**AI tools that demand complete upfront specification are making the same mistake as waterfall development.**

Iterative interaction isn't a workaround for the expressibility gap.

It's how human cognition **actually works**.

This is backed by research from:
- Herbert Simon (bounded rationality)
- Rittel & Webber (wicked problems)
- Donald Schön (reflection-in-action)
- Dave Snowden (Cynefin framework)

**Read the full theoretical framework**: [github.com/MartyBonacci/expressibility-gap](https://github.com/MartyBonacci/expressibility-gap)
